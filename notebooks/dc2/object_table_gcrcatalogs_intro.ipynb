{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubin LSST DESC DC2: Accessing Object Table with GCRCatalogs\n",
    "\n",
    "**Authors**: Yao-Yuan Mao (@yymao), Francois Lanusse (@EiffL), Javier Sanchez (@fjaviersanchez), Michael Wood-Vasey (@wmwv), Rachel Mandelbaum (@rmandelb)\n",
    "\n",
    "This notebook will illustrate the basics of accessing the Object Table, which contains the detected objects at the coadd level using GCRCatalogs.\n",
    "\n",
    "**Learning objectives**: After going through this notebook, you should be able to:\n",
    "  1. Load and efficiently access a DC2 object table with the GCRCatalogs\n",
    "  2. Understand and have references for the object table schema\n",
    "  3. Apply cuts to the catalog using GCRQuery\n",
    "  4. Have an example of quality cuts and simple star/galaxy separation cut"
   ]
  },
  {
   "source": [
    "## Before you start\n",
    "\n",
    "Make sure you have followed the instructions [here](https://lsstdesc.org/DC2-Public-Release/) to \n",
    "download the data files, install `GCRCatalogs`, and set up `root_dir` for `GCRCatalogs`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GCRCatalogs\n",
    "from GCRCatalogs.helpers.tract_catalogs import tract_filter, sample_filter\n",
    "from GCRCatalogs import GCRQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access object table with GCRCatalogs\n",
    "\n",
    "The [GCRCatalogs](https://github.com/LSSTDESC/gcr-catalogs) package is a DESC project which aims at gathering in one convenient location various simulation/data catalogs made available to the collaboration. In this section, we illustrate how to use this tool to access the object catalogs from DC2 Run2.2i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCRCatalogs.get_public_catalog_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cat = GCRCatalogs.load_catalog(\"desc_dc2_run2.2i_dr6_object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Table schema\n",
    "\n",
    "To see the \"columns\" (sometimes called \"quantities\") available in the object table, you can call `list_all_quantities()`.\n",
    "The returned list of `list_all_quantities` is not sorted. Sorting it would make it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(obj_cat.list_all_quantities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions, units, and types of these fields are documented in the data release note (Table 1).\n",
    "As explained in the note, the values exposed here are not the native columns produced by the LSST Science Pipelines.\n",
    "Instead, this schema strives to mimic the schema of the LSST Data Products Definition Document [DPDD](http://ls.st/dpdd), \n",
    "which is an effort made by the Rubin Observatory to standardize the format of the official Data Release Products (DRP).\n",
    "\n",
    "If the data release note is many clicks away, you can also check the definitions, units, and types of these columns\n",
    "using `get_quantity_info()`. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cat.get_quantity_info(\"Iyy_pixel_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the catalog includes:\n",
    "* Positions\n",
    "* Fluxes and magnitudes (PSF and [CModel](https://www.sdss.org/dr12/algorithms/magnitudes/#cmodel))\n",
    "* Adaptive second moments (using [GalSim's HSM](http://galsim-developers.github.io/GalSim/namespacegalsim_1_1hsm.html))\n",
    "* Quality flags: e.g, does the source have any interpolated pixels? Has any measurement algorithm returned an error?\n",
    "* Other useful quantities: `blendedness`, measure of how flux is affected by neighbors: (1 - flux.child/flux.parent) (see section 4.9.11 of [Bosch et al. 2018](http://adsabs.harvard.edu/abs/2018PASJ...70S...5B)); `extendedness`, classifies sources as extended or point source (see section 4.9.10 of [Bosch et al. 2018](http://adsabs.harvard.edu/abs/2018PASJ...70S...5B))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data in an interactive session\n",
    "\n",
    "The DC2 DR6 is a very large data set; loading the full object table into memory is usually not feasible. To access the data in a memory efficient fashion while maintaining reasonable performace, we usually need to do one or a combination of the following measures. \n",
    "\n",
    "1. Loading only the columns that you immediately need\n",
    "2. Loading a subset of the sky area that you immediately need, or iterating over sky areas (\"tracts\"; see below)\n",
    "3. Sampling the table at random if your science allows\n",
    "\n",
    "Because the physical data files are in Parquet format, loading specific columns (1) is efficient and does not require reading in the full table. \n",
    "The physical data files are also partitioned in \"tracts\", each of which corresponds to a region of the sky (see Figure 1 of the release note); hence, loading only a subset of \"tracts\" or iterating over tracts (2) will also make the memory footprint much smaller. \n",
    "Finally, subsampling the table at random can make your analysis faster and use less memory, but it does not save much in term of I/O as all the rows need to be read in before subsampling.\n",
    "\n",
    "\n",
    "#### If you use GCRCatalogs to access the data\n",
    "\n",
    "You will use `get_quantities` to retrieve the data, and there are several convenient, high-level features that help you achieve the three measures described above. When calling `get_quantities`, you can\n",
    "\n",
    "1. Specify the columns to load\n",
    "2. Add `tract_filter(tracts)` to `native_filters` to select tracts, and use `return_iterator` to iterate over tracts\n",
    "3. Add `sample_filter(frac)` to `filters` to sample the data frame at random\n",
    "\n",
    "Here `tract_filter` and `sample_filter` are helper functions that can be imported from `GCRCatalogs.helpers.tract_catalogs`.\n",
    "We will demostrate all these features in this notebook.\n",
    "\n",
    "\n",
    "#### If you use pandas to read the data files directly\n",
    "\n",
    "You can achieve the three measures described above by:\n",
    "\n",
    "1. Specifying the `columns` argument when calling `pandas.read_parquet`\n",
    "2. Reading in one file (i.e., one tract) at a time, and only reading in the tracts you need\n",
    "3. Using `pandas.DataFrame.sample` to sample the data frame at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GCRCatalogs' `get_quantities`\n",
    "\n",
    "#### Call signature\n",
    "\n",
    "```python\n",
    "obj_cat.get_quantities(quantities=[...], filters=[...], native_filters=[...], return_iterator=True/False)\n",
    "```\n",
    "\n",
    "You can supply:\n",
    "* a list of column names of those you want to load to `quantities`. \n",
    "* a list of `GCRQuery`s (including `sample_filter`'s return and other quality cuts (see later part of this notebook)) to `filters`\n",
    "* a list of tract-selecting `GCRQuery`s (i.e., those returned by  `tract_filter`) to `native_filters`\n",
    "* `True` to `return_iterator` if you want to iterate over tracts, or `False` (default) if you want to concatenate data across tracts. \n",
    "\n",
    "`get_quantities` will return (or yield, if `return_iterator=True`) a Python dictionary that contains\n",
    "columns names as keys and corresponding column arrays as values. \n",
    "\n",
    "We will learn more about `GCRQuery` and `return_iterator` latter. For now, let's first learn how to use the two helper functions that we imported earlier: `tract_filter` and `sample_filter`. \n",
    "\n",
    "#### Using `tract_filter`\n",
    "\n",
    "There are three ways to call `tract_filter`\n",
    "\n",
    "```python\n",
    "tract_filter(4225)  # select one tract, 4225\n",
    "tract_filter(4200, 4300)  # select all tracts between 4200 and 4300 (inclusive on both ends!)\n",
    "tract_filter([4225, 4228, 4230])  # select the tracts specified in the list\n",
    "```\n",
    "\n",
    "You can then directly use the returns in the `native_filters` argument of `get_quantities`. Note that these tract filters will select on matched tracts, but will *not* warn or raise errors if a wanted tract is not available. \n",
    "\n",
    "`obj_cat.available_tracts` will give you a list of tracts that are available (i.e., the ones you have downloaded). If you defined a tract filter, you can also preview the list of tracts that satisfy the filter. See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obj_cat.available_tracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_filter(2900, 3000).filter({\"tract\": np.array(obj_cat.available_tracts)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `sample_filter`\n",
    "\n",
    "There are two ways to call `sample_filter`\n",
    "\n",
    "```python\n",
    "sample_filter(0.2)  # select 20% of entries at random\n",
    "sample_filter(0.01, random_state=1234)  # select 1% of entries at random, but using a fixed random seed 1234\n",
    "```\n",
    "\n",
    "You can then directly use the returns in the `filters` argument  of `get_quantities`.\n",
    "\n",
    "Now let's look at an example! We would like to retrieve the coordinates (ra, dec) of all objects in the tracts 4225, 4226, and 4430, and apply a down-sampling fraction of 1%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = obj_cat.get_quantities(\n",
    "    quantities=['ra', 'dec'],                            # columns we want to load, \n",
    "    filters=[sample_filter(0.01)],                       # down sample at 1%\n",
    "    native_filters=[tract_filter([4225, 4226, 4430])],   # select three tracts\n",
    "    return_iterator=False,                               # concatenate data across tracts \n",
    ")\n",
    "\n",
    "# Plot a 2d histogram of sources \n",
    "fig, ax = plt.subplots(figsize=(10,6), dpi=100)\n",
    "im = ax.hist2d(data['ra'], data['dec'], 100)[-1]\n",
    "ax.set_xlabel('RA [deg]');\n",
    "ax.set_ylabel('Dec [deg]');\n",
    "plt.colorbar(im, ax=ax, label='1% of total number of objects per bin');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returned data of `get_quantities`\n",
    "\n",
    "As mentioned above, the data returned by the GCR is structured as a native Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can also easily be converted into a [Pandas DataFrame](https://pandas.pydata.org), if you are so inclined ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or an [astropy table](https://docs.astropy.org/en/stable/table/) too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.table\n",
    "\n",
    "table = astropy.table.Table(data)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `return_iterator` when calling `get_quantities`\n",
    "\n",
    "Concatenating data across tracts is convenient but costs more memory. If your analysis allows, a more memory efficient approach would be to reduce the data for one tract, stored the reduced product, and repeat for other tracts. The `return_iterator` option will turn `get_quantities` into a generator function (i.e., can used in a for loop directly, like `range`), making it easy for users to iterate over tracts.\n",
    "\n",
    "In the example below, we will loop over ten tracts that have the same RA range (you can look up tract numbers in Figure 1 of the release note).\n",
    "For each tract, we will load 0.1% of the objects and put them on a scatter plot. We will also print out a message to see where we are in the for loop. When the loop finishes, we should see a beautiful and colorful strip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "\n",
    "for data in obj_cat.get_quantities(                     # note how we use `get_quantities` directly in a for loop\n",
    "    quantities=['ra', 'dec', 'tract'],                  # columns we want to load, \n",
    "    filters=[sample_filter(0.001)],                     # down sample at 0.1%\n",
    "    native_filters=[tract_filter(3075, 3084)],          # select 10 tracts [3075, 3084] inclusive on ends\n",
    "    return_iterator=True,                               # return an iterator that iterates over tracts\n",
    "):\n",
    "    print(\"Plotting data from tract\", data[\"tract\"][0])  # print out which tract is current being plotted\n",
    "    ax.scatter(data['ra'], data['dec'], s=1, label=str(data[\"tract\"][0]));\n",
    "    # You can put more science here! \n",
    "\n",
    "ax.set_xlabel('RA [deg]');\n",
    "ax.set_ylabel('Dec [deg]');\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"Tract\", handletextpad=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying more filters (quality cuts or selection cuts)\n",
    "\n",
    "So far we have only used the sampler in the `filters` option of `get_quantities`. \n",
    "In a more realistic setting, you likely would want to apply certain quality cuts or selection cuts to your data. \n",
    "You can of course apply these cuts _after_ you read in the data, especially if you want to play with different cut choices. \n",
    "On the other hand, if you already know certain quality cuts or selection cuts that you always want to apply, \n",
    "then applying them at data load time (i.e., when calling `get_quantities`) can save some memory for you! \n",
    "\n",
    "*Note*: If you want to apply cuts after reading in the data, it would be easier to first convert the returned data \n",
    "(which is a python dictionary) into a pandas data frame or an astropy table (see instructions in the earlier part of this notebook), \n",
    "and then apply the cuts. This way, all columns will be filtered with the same cuts at once. \n",
    "\n",
    "To apply cuts at data load time, you first need to specify your quality cuts or selection cuts using `GCRQuery`, \n",
    "and then add them to the `filters` option of `get_quantities`.\n",
    "You can read more about how to specify the queries [here](https://github.com/yymao/easyquery#usage), \n",
    "but we will show you some common examples below.\n",
    "\n",
    "Each query should be a string that contains a *simple* python expression (see [numexpr doc](https://numexpr.readthedocs.io/en/latest/user_guide.html#supported-operators) for supported operators and functions), in which the variables are column names. \n",
    "The expression should evaluate to a boolean array. \n",
    "\n",
    "Alternatively, a query can also be in the form of `GCRQuery((func, col1, col2, ...))` (note the two pairs of parentheses).\n",
    "In this form, `col1`, `col2`, ... should be column names, and `func(d[col1], d[col2], ...)` should return a boolean array\n",
    "that has the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic query constructions\n",
    "\n",
    "GCRQuery('clean')  # selects `clean` == True  -- note that clean is a boolean array itself\n",
    "GCRQuery('extendedness == 0')  # selects `extendedness` == 0\n",
    "GCRQuery((np.isfinite, 'mag_r_cModel'))  # selects `mag_r_cModel` is finite\n",
    "GCRQuery((np.isfinite, 'mag_r_cModel'), \"mag_r_cModel < 25\")  # selects `mag_r_cModel` is finite *AND* mag_r_cModel < 25\n",
    "GCRQuery(\"mag_g_cModel - mag_r_cModel < 1\")  # selects `mag_g_cModel` - `mag_r_cModel` < 1\n",
    "\n",
    "\n",
    "# You can name the queries, and do logical operations (AND &, OR |, XOR ^, NOT ~) on them\n",
    "\n",
    "is_extended = GCRQuery('extendedness == 1')\n",
    "clean = GCRQuery('clean')\n",
    "not_clean = ~clean\n",
    "finite_g = GCRQuery((np.isfinite, 'mag_g_cModel'))\n",
    "finite_r = GCRQuery((np.isfinite, 'mag_r_cModel'))\n",
    "bright_g = finite_g & GCRQuery(\"mag_g_cModel < 25\")\n",
    "bright_r = finite_r & GCRQuery(\"mag_r_cModel < 25\")\n",
    "bright_g_or_r = bright_g | bright_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you defined these queries, you can just put them in a list that you supply to the `filters` option of `get_quantities`. \n",
    "\n",
    "All queries in a lists will be joined by *AND*. If `a`, `b`, and `c` are all `GCRQuery` objects, then:\n",
    "\n",
    "- `filters=[a, b, c]` is the same as `filters=[a & b & c]`\n",
    "- `filters=[a, b | c]` means selecting on `a` AND (`b` OR `c`)\n",
    "\n",
    "\n",
    "Specifying `filters` when calling `get_quantities` is similar to slicing the data based on the `GCRQuery` that you supply. \n",
    "For example, the following three methods will all produce the same data frames:\n",
    "\n",
    "```python\n",
    "# method1\n",
    "df = pd.DataFrame(obj_cat.get_quantities([\"ra\", \"dec\"], filters=[GCRQuery(\"mag_g_cModel - mag_r_cModel < 1\")]))\n",
    "\n",
    "# method2\n",
    "df = pd.DataFrame(obj_cat.get_quantities([\"ra\", \"dec\", \"mag_g_cModel\", \"mag_r_cModel\"]))\n",
    "df = df.query(\"mag_g_cModel - mag_r_cModel < 1\")\n",
    "df = df[\"ra\", \"dec\"]\n",
    "\n",
    "# method3\n",
    "df = pd.DataFrame(obj_cat.get_quantities([\"ra\", \"dec\", \"mag_g_cModel\", \"mag_r_cModel\"]))\n",
    "df = df[df[\"mag_g_cModel\"] - df[\"mag_r_cModel\"] < 1]\n",
    "df = df[\"ra\", \"dec\"]\n",
    "```\n",
    "\n",
    "Note that when using `filters`, you don't need to add the quantities, that are only used in the filter queries to the `quantities` argument. \n",
    "If you do want to keep those quantities, remember to add them to `quantities`  too. \n",
    "For those who are familiar with SQL, the behavior here is analogous to `SELECT <quantities> WHERE <filters>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to store the reduced histogram during the iteration \n",
    "color_bins = np.linspace(-2, 3, 101)\n",
    "hist = np.zeros(len(color_bins)-1, dtype=np.int)\n",
    "\n",
    "# iterate over tract using get_quantities(..., return_iterator=True)\n",
    "for data in obj_cat.get_quantities(\n",
    "    quantities=['mag_g_cModel', 'mag_r_cModel'],                            # columns we want to load, \n",
    "    filters=[sample_filter(0.1), clean, is_extended, bright_g | bright_r],  # filters we want, including the sampler\n",
    "    native_filters=[tract_filter([4225, 4226, 4430])],                      # select three tracts\n",
    "    return_iterator=True,                                                   # return an iterator\n",
    "):\n",
    "    gr = data[\"mag_g_cModel\"] - data[\"mag_r_cModel\"]\n",
    "    hist += np.histogram(gr, color_bins)[0]  # accumulate the histogram\n",
    "\n",
    "# Plot a 1d histogram of colors \n",
    "fig, ax = plt.subplots(figsize=(8,5), dpi=100)\n",
    "ax.bar(color_bins[:-1], hist, width=np.ediff1d(color_bins))\n",
    "ax.set_xlabel(\"Galaxy color $g-r$ [mag]\");\n",
    "ax.set_ylabel(\"10% of counts per bin\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Science examples\n",
    "\n",
    "Finally, we will show some science examples that combine what we have learned so far. \n",
    "These examples can be used as your own analyses!\n",
    "\n",
    "### Galaxy number density\n",
    "\n",
    "We have `extendedness` as a tool for star/galaxy classification. An object is considered extended if the the difference between the `PSF` magnitude and the [`CModel` magnitude](https://www.sdss.org/dr12/algorithms/magnitudes/#cmodel) is above a certain threshold (0.0164). To learn more about this see [Bosch et al. 2018](http://adsabs.harvard.edu/abs/2018PASJ...70S...5B) section 4.9.10.\n",
    "\n",
    "Let's first dfine a few useful cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_extended = GCRQuery('extendedness == 1')  # Extended objects (primarily galaxies)\n",
    "clean = GCRQuery('clean')  # The source has no flagged pixels (interpolated, saturated, edge, clipped...) \n",
    "                           # and was not skipped by the deblender\n",
    "\n",
    "galaxy_cuts = [\n",
    "    clean,\n",
    "    is_extended, \n",
    "    GCRQuery((np.isfinite, 'mag_i_cModel')),  # Select objects that have i-band cmodel magnitudes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the data from just one tract to save run time. \n",
    "To make a galaxy number density plot (as a function of magnitude), we need the galaxy magnitude, and RA/Dec to calculate the sky area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns we need -- note that you don't need to include the columns used in your cuts if you don't need them\n",
    "# for purposes other than the cuts!\n",
    "columns = [\"ra\", \"dec\", \"mag_i_cModel\"]\n",
    "\n",
    "data = obj_cat.get_quantities(\n",
    "    quantities=columns, \n",
    "    filters=galaxy_cuts, \n",
    "    native_filters=[tract_filter(3830)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see earlier, the geometry of a tract is rectangular, so here we are going to use naive area estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ra = data[\"ra\"].max() - data[\"ra\"].min()\n",
    "d_dec = data[\"dec\"].max() - data[\"dec\"].min()\n",
    "cos_dec = np.cos(np.deg2rad(np.median(data[\"dec\"])))\n",
    "sky_area_sq_arcmin = (d_ra * cos_dec * 60) * (d_dec * 60)\n",
    "print(sky_area_sq_arcmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_bins = np.linspace(20, 30, 51)\n",
    "\n",
    "cdf = np.searchsorted(data[\"mag_i_cModel\"], mag_bins, sorter=data[\"mag_i_cModel\"].argsort())\n",
    "\n",
    "plt.semilogy(mag_bins, cdf / sky_area_sq_arcmin)\n",
    "plt.xlabel(\"$i$-band cModel mag\");\n",
    "plt.ylabel(\"Cumulative number per sq. arcmin\");\n",
    "\n",
    "plt.grid();  # add grid to guide our eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stellar locus\n",
    "\n",
    "Let's now take a look at the colors of objects that we think are stars. We will do this on only one tract to save run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_cuts = [\n",
    "    clean,\n",
    "    ~is_extended,\n",
    "    GCRQuery((np.isfinite, 'mag_g')), # Have finite point-source model magnitudes \n",
    "    GCRQuery((np.isfinite, 'mag_r')),\n",
    "    GCRQuery((np.isfinite, 'mag_i')),\n",
    "]\n",
    "\n",
    "# columns we need -- note that you don't need to include the columns used in your cuts if you don't need them\n",
    "# for purposes other than the cuts!\n",
    "columns = ['mag_g', 'mag_r', 'mag_i']\n",
    "\n",
    "# loading the data\n",
    "data = obj_cat.get_quantities(\n",
    "    quantities=columns, \n",
    "    filters=star_cuts, \n",
    "    native_filters=[tract_filter(3830)]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,6), dpi=100)\n",
    "im = ax.hexbin(\n",
    "    data['mag_g']-data['mag_r'],\n",
    "    data['mag_r']-data['mag_i'],\n",
    "    bins='log', \n",
    "    extent=[-1,2,-1,2],\n",
    ")\n",
    "ax.set_xlabel('$g-r$ [mag]');\n",
    "ax.set_ylabel('$r-i$ [mag]');\n",
    "plt.colorbar(im, ax=ax, label='log(Number of objects)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9c54e8edd505ba0c12578c8976d22821e1092c9eec5194059a76e3a4e896bf91"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}